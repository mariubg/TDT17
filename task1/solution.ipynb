{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task a) Compute $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to perform the convolution. We call the output of the convolutional layer $CN$:\n",
    "\n",
    "$$\\begin{align}CN  & = X \\circledast F^C - B^C\\\\\n",
    "\n",
    "& = \\begin{bmatrix}\n",
    "0 & 3 & -1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 3 & 0 \\\\\n",
    "3 & 0 & 3 & 1\n",
    "\\end{bmatrix} \\circledast\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 0 \\\\\n",
    "\\end{bmatrix} -1\\\\\n",
    "\n",
    "& = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-2 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put this matrix through the self-attention layer. \n",
    "\n",
    "First we calculate the query, key and value matrices:\n",
    "$$\n",
    "Q = CN * W^Q\n",
    "= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-2 & 1 \\\\\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "0 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-2 & 6 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "K = CN * W^K\n",
    "= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-2 & 1 \\\\\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "3 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "1 & 2 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "V = CN * W^V\n",
    "= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-2 & 1 \\\\\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-7 & -1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "\n",
    "The output of the self-attention layer is:\n",
    "$$SA = softmax(\\frac{Q * K^T}{\\sqrt{d}})* V$$\n",
    "\n",
    "d is the size of dimaensions of each query, in this network 2. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "SA & = softmax(\\frac{\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-2 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "1 & 2 \\\\\n",
    "\\end{bmatrix}^T\n",
    "}{\\sqrt{2}})*\n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-7 & -1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "& = softmax(\\frac{\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-2 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "-1 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "}{\\sqrt{2}})*\n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-7 & -1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "& = softmax(\\begin{bmatrix}\n",
    "-\\frac{1}{\\sqrt{2}} & -\\frac{5}{\\sqrt{2}} \\\\\n",
    "2\\cdot\\sqrt{2} & 7\\cdot\\sqrt{2} \\\\\n",
    "\\end{bmatrix})*\n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-7 & -1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "& = \\begin{bmatrix}\n",
    "0.03 & 0 \\\\\n",
    "0.97 & 1 \\\\\n",
    "\\end{bmatrix}*\n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "-7 & -1 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "& = \\begin{bmatrix}\n",
    "0.09 & 0.03 \\\\\n",
    "-4.09 & 1.97 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\end{align}$$\n",
    "\n",
    "Now we flatten this so that $SA = \\begin{bmatrix}\n",
    "0.09\\\\\n",
    "0.03\\\\\n",
    "-4.09\\\\\n",
    "1.97\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Finally, we calculate $\\hat{y}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} & = ReLU(W^{DT}* SA -B^D)\\\\\n",
    "& = ReLU(\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\\\\\n",
    "\\end{bmatrix}^T * \\begin{bmatrix}\n",
    "0.09\\\\\n",
    "0.03\\\\\n",
    "-4.09\\\\\n",
    "1.97\\\\\n",
    "\\end{bmatrix} - 1)\\\\\n",
    "& = ReLU(\\begin{bmatrix}\n",
    "1 & 1 & 0 & 2\\\\\n",
    "\\end{bmatrix}*\\begin{bmatrix}\n",
    "0.09\\\\\n",
    "0.03\\\\\n",
    "-4.09\\\\\n",
    "1.97\\\\\n",
    "\\end{bmatrix} - 1)\\\\\n",
    "\n",
    "& = ReLU(3.06)\\\\\n",
    "\n",
    "& = \\underline{\\underline{3.06}}\n",
    "\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task b) Update the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we calculate the derivative of the error:\n",
    "$$\\frac{\\partial C}{\\partial \\hat{y}} = 2(y-\\hat{y}) = -4.12$$\n",
    "\n",
    "To update the weights and biases of the FCNN, we propagate it back:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial W^D} & = \\frac{\\partial C}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W^D}\\\\\n",
    "& = 2(y-\\hat{y})\\cdot SA\n",
    "& = -4.12 \\cdot \\begin{bmatrix}\n",
    "0.09\\\\\n",
    "0.03\\\\\n",
    "-4.09\\\\\n",
    "1.97\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}\n",
    "-0.37\\\\\n",
    "-0.12\\\\\n",
    "16.85\\\\\n",
    "-8.12\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial B^D} = \\frac{\\partial C}{\\partial \\hat{y}} = -4.12\n",
    "$$\n",
    "\n",
    "We can now update $W^D$ and $B^D$:\n",
    "\n",
    "$$\n",
    "W_1^D = W_0^D -\\alpha\\frac{\\partial C}{\\partial W^D} = \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\\\\\n",
    "\\end{bmatrix} - 0.1\n",
    "\\begin{bmatrix}\n",
    "-0.37\\\\\n",
    "-0.12\\\\\n",
    "16.85\\\\\n",
    "-8.12\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1.04\\\\\n",
    "1.01\\\\\n",
    "-1.69\\\\\n",
    "2.812\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "B_1^D = B_0^D - \\alpha\\frac{\\partial C}{\\partial B^D} = -1 - 0.1\\cdot -4.12 = 0.59\n",
    "$$\n",
    "\n",
    "In order to calculate the derivatives for the other layers, we also need:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial C}{\\partial SA} & = \\frac{\\partial C}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial SA}\\\\\n",
    "& = 2(y-\\hat{y}) \\cdot W^D\n",
    "& = -4.12 \\cdot \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix}\n",
    "-4.12\\\\\n",
    "-4.12\\\\\n",
    "0\\\\\n",
    "-8.24\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We reshape this to: $\\frac{\\partial C}{\\partial SA} = \\begin{bmatrix}\n",
    "-4.12 & -4.12\\\\\n",
    "0 & -8.24\\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Backpropagation through the self-attention layer is a bit more complicated. In order to make it more readable, we call $\\frac{Q* K^T}{\\sqrt{d}}$ $S$, and $softmax(S)$ $A$, so that $SA = A* V = softmax(S)* V$. Then we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial V} & = A^T * \\frac{\\partial C}{\\partial SA}\\\\\n",
    "& = \\begin{bmatrix}\n",
    "0.03 & 0.97 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix} * \\begin{bmatrix}\n",
    "-4.12 & -4.12\\\\\n",
    "0 & -8.24\\\\\n",
    "\\end{bmatrix}\n",
    "& = \\begin{bmatrix}\n",
    "-0.12 & -8.12\\\\\n",
    "0 & -8.24\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be used to calculate the derivatives of the cost wrt teh values weights:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial C}{\\partial W^V} & = CN^T * \\frac{\\partial C}{\\partial V}\\\\\n",
    "& = \\begin{bmatrix}\n",
    "1 & -2\\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix} * \\begin{bmatrix}\n",
    "-0.12 & -8.12\\\\\n",
    "0 & -8.24\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}\n",
    "-0.12 & 8.36\\\\\n",
    "0 & -8.24\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now we update the value weights:\n",
    "$$\n",
    "W_1^V = W_0^V - \\alpha\\cdot\\frac{\\partial C}{\\partial W^V} = \n",
    "\\begin{bmatrix}\n",
    "3.01 & 0.17\\\\\n",
    "-1 & 1.82\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
